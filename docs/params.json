{
  "name": "dCpp",
  "tagline": "Differentiable C++; conditionals, loops, recursion and all things C++",
  "body": "# dCpp\r\nDifferentiable C++; conditionals, loops, recursion and all things C++\r\n\r\n###Abstract\r\nAlgorithms were defined as a non-Abelian subgroup (or more generally a submonoid) of operators acting on a virtual space. By defining a norm on this space, and the notion of a limit, we show the operators to be differentiable through τ − calculus developed by the author. Thus all techniques of Functional analysis may be applied to any algorithm. \r\n\r\nLibrary dC++ implements the theory of τ − calculus in a natural way, needing only replacement of the type double with a differentiable variable [var](/include/var.h). Thus all C++ written code is compliable with the library. dC++ supports all external C++ libraries written in generic programming paradigm.\r\n\r\nThis is the openSource version.\r\n\r\n###Usage\r\nAs algorithms become differentiable, so does a whole new set of procedures. Algorithms involving solving equations, approximating models, SVD decomposition, eigen vector/value calculation, etc. as one of their steps, Runge-Kutta and other approximation methods, all become differentiable and can be subjected to standard methods of Functional analysis, thus providing exact derivatives of analyticaly non-computable expressions.\r\n\r\n###Tutorial\r\nAs most programmers face the need of differentiability through machine learning, we use the concept of a [Reccurent Neural Network](https://en.wikipedia.org/wiki/Recurrent_neural_network) as a vessel for this tutorial.\r\n\r\nWe will create a simple 2-layers deep Neural Network with 10 recursive layers. 20 layers all together. Each layer will have sigmoid activation functions, ending with a softmax layer, producing a distribution. We will code all the functions as we go (even though much of such functionality comes with the library), for educational purposes.\r\n\r\nFirst we include the necessities\r\n\r\n```c++\r\n#include <iostream>\r\n#include <dC.h>\r\n```\r\nWe will need the folowing functions\r\n* [e(x)](https://en.wikipedia.org/wiki/Exponential_function)\r\n* [sigmoid(x)](https://en.wikipedia.org/wiki/Sigmoid_function)\r\n* [softmax(vec)](https://en.wikipedia.org/wiki/Softmax_function)\r\n* [dotProduct(vec1,vec2)](https://en.wikipedia.org/wiki/Dot_product)\r\n* [matVecProduct(mat,vec)](https://en.wikipedia.org/wiki/Matrix_multiplication)\r\n\r\nBy coding e(x), we will learn about the class [tau](include/tau.h), which allows one to create it's own differentiable maps, returning a differentiable variable [var](/include/var.h).\r\nFirst we create maps double->double, for e(x) and its' derivative.\r\n```c++\r\n//y=e^x\r\ndouble eMap(double x){\r\n    return std::exp(x);\r\n}\r\n\r\n//dy/dx=e^x\r\ndouble deMap(double x){\r\n    return std::exp(x);\r\n}\r\n```\r\nWe create a differentiable map e(x), by providing [tau](include/tau.h) with the above functions.\r\n```c++\r\n//create a differentiable mapping\r\ntau e=tau(eMap,deMap);\r\n```\r\nUsing this, we may code the sigmoid map.\r\n```c++\r\n//sigmoid map\r\nvoid sigmoid(var *v,int n){\r\n    for(int i=0;i<n;i++){\r\n        v[i]=1/(1+e(-v[i]));\r\n    }\r\n}\r\n```\r\nNow the softmax map.\r\n```c++\r\nvoid softmax(var *v, int n){\r\n    var sum=var(0);\r\n    for(int i=0;i<n;i++){\r\n        v[i]=e(v[i]);\r\n        sum+=v[i];\r\n    }\r\n    for(int i=0;i<n;i++) {\r\n        v[i]/=sum;\r\n    }\r\n}\r\n```\r\nNext, we code the dot product and the matrix vector multiplication. For simplicity dimensions are specific to the example.\r\n```c++\r\n//dot product mapping\r\nvar dotProduct(var *v1,var *v2, int n){\r\n    var out=var(0);\r\n    for(int i=0;i<n;i++){\r\n        out+=v1[i]*v2[i];\r\n    }\r\n    return out;\r\n}\r\n//computes M*x\r\nvoid matVecProduct(var *out,var mat[][2],var *vec, int size1,int size2){\r\n    for(int i=0;i<size1;i++){\r\n        out[i]=dotProd(mat[i],vec,size2);\r\n    }\r\n}\r\n```\r\nWe have all the tools needed to build a recursive layer. It will consist of two layers, mapping a 2-vector to a 2-vector. Output of the second layer will be recursively connected to the input of the next recursive layer.\r\n\r\n```c++\r\nvoid recursionNet(var *input, var layer1[2][2],var layer2[2][2],var *result, int depth){\r\n    if(depth==0){\r\n        for(int i=0;i<2;i++){\r\n            result[i]=input[i];\r\n        }\r\n        softmax(result,2);\r\n    }\r\n    else{\r\n        var firstOut[2];\r\n        //matrix vector multiplication\r\n        matVecProduct(firstOut,layer1,input,2,2);\r\n        sigmoid(firstOut,2);\r\n        var secondOut[2];\r\n        matVecProduct(secondOut,layer2,firstOut,2,2);\r\n        sigmoid(secondOut,2);\r\n        recursionNet(secondOut,layer1,layer2,result,depth-1);\r\n    }\r\n}\r\n```\r\nAll that is left, is the main function in which we will learn about the initialization of differentiable variables [var](/include/var.h). Note, how we do not initialize the input vector, but we do initialize the weight matricies, as only their derivatives are of interest.\r\nFinally, we call recursionNet, for a depth of 10 layers and display its' Jacobian and image.\r\n```c++\r\nint main(){\r\n    //create input vector\r\n    var input[2];\r\n    for(int i=0;i<2;i++){\r\n        input[i]=var(i+1);\r\n    }\r\n    //create initial matrix\r\n    var firstMat[2][2];\r\n    for(int i=0;i<2;i++){\r\n        for(int j=0;j<2;j++){\r\n            firstMat[i][j]=var((i+1)*(j+1));\r\n        }\r\n    }\r\n    //initialize all elements\r\n    for(int i=0;i<2;i++){\r\n        for(int j=0;j<2;j++){\r\n            dC::init(firstMat[i][j]);\r\n        }\r\n    }\r\n    //create inital matrix for second layer\r\n    var secondMat[2][2];\r\n    for(int i=0;i<2;i++){\r\n        for(int j=0;j<2;j++){\r\n            secondMat[i][j]=var((j+1)*(i+1));\r\n        }\r\n    }\r\n    //initialize all elements\r\n    for(int i=0;i<2;i++){\r\n        for(int j=0;j<2;j++){\r\n            dC::init(secondMat[i][j]);\r\n        }\r\n    }\r\n    var out[2];\r\n    //run for 10 recursive steps\r\n    recursionNet(input,firstMat,secondMat,out,10);\r\n    //display the image and the Jacobian\r\n    for(int i=0;i<2;i++){\r\n        dC::print(out[i]);\r\n    }\r\n}\r\n```\r\n\r\n###External libraries\r\n\r\nUsage with external libraries written in generic paradigm is demonstrated on the example of [Eigen](http://eigen.tuxfamily.org/). \r\nWe will code a perceptron with sigmoid activations, followed by softmax normalization, taking 28x28 image as an input and outputting a 10 class classifier. We will use dC++ provided mappings.\r\n\r\n```c++\r\n#include <iostream>\r\n#include <dC.h>\r\n\r\nusing namespace std;\r\nusing namespace dC;\r\n\r\n//create a softmax function\r\ntemplate <typename Derived>\r\n    void softmax(Eigen::MatrixBase<Derived>& matrix){\r\n            //maps each element of the matrix by y=e^x;\r\n            dC::map_by_element(matrix,&dC::exp);\r\n            //sums the elements of the matrix using Eigens function\r\n            var tmp=matrix.sum();\r\n            //divides each element by the sum\r\n            for (size_t i=0, nRows=matrix.rows(), nCols=matrix.cols(); i<nCols; ++i)\r\n                for (size_t j=0; j<nRows; ++j){\r\n                    matrix(j,i)/=tmp;\r\n                }\r\n}\r\n\r\nint main(){\r\n    //    Matrix holding the inputs (imgSizeX1 vector)\r\n    const int imgSize=28*28;\r\n    const Eigen::Matrix<var,1,imgSize>input=Eigen::Matrix<var,1,imgSize>::Random(1,imgSize);\r\n    //    number of outputs of the layer\r\n    const int numOfOutOnFirstLevel=10;\r\n    //    matrix of weights on the first level (imgSizeXnumOfOutOnFirstLevel)\r\n    Eigen::Matrix<var,imgSize,numOfOutOnFirstLevel>firstLayerVars=Eigen::Matrix<var,imgSize,numOfOutOnFirstLevel>::Random(imgSize,numOfOutOnFirstLevel);\r\n    //    initializing weights\r\n    dC::init(firstLayerVars);\r\n    //    mapping of the first layer --> resulting in 10x1 vector\r\n    Eigen::Matrix<var,numOfOutOnFirstLevel,1>firstLayerOutput=input*firstLayerVars;\r\n    //    apply sigmoid layer --> resulting in 10x1 vector\r\n    dC::map_by_element(firstLayerOutput,&dC::sigmoid);\r\n    //    apply sofmax layer --> resulting in 10x1 vector\r\n    softmax(firstLayerOutput);\r\n    //    display the first output layer and its Jaccobian\r\n    //    Jacobian is a 10x7840 matrix of derivatives\r\n    for (size_t i=0, nRows=firstLayerOutput.rows(), nCols=firstLayerOutput.cols(); i<nCols; ++i){\r\n                for (size_t j=0; j<nRows; ++j) {\r\n                    dC::print(firstLayerOutput(j,i));\r\n                }\r\n                cout<<endl;\r\n    }\r\n}\r\n\r\n```\r\n\r\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">dC++</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"https://si.linkedin.com/in/zigasajovic\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Žiga Sajovic</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>.\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}